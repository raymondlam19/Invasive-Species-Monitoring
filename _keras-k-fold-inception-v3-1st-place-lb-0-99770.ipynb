{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "0d16d13f-57b1-42e6-9c75-64c7d2d7d5fe",
    "_uuid": "4cf432a0d22e9e6d1d5261bfd63d9fe7ad03fe88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "20db2f1a-471f-4092-8692-9ec8bd0ee1de",
    "_uuid": "65653e763d41204bc764f7e1151829b735e3fb91"
   },
   "source": [
    "# Load Datasets\n",
    "Since we will be using a generator we don't need to actually load in any files into memory, all we need is the filepaths :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "36d7aa24-8652-4a54-9993-79e305405312",
    "_uuid": "5a87f8e975cf12b89ac6a966adee3b8b73249b52"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./input/train/1.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./input/train/2.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./input/train/3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./input/train/4.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./input/train/5.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  invasive\n",
       "0  ./input/train/1.jpg         0\n",
       "1  ./input/train/2.jpg         0\n",
       "2  ./input/train/3.jpg         1\n",
       "3  ./input/train/4.jpg         0\n",
       "4  ./input/train/5.jpg         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./input/train/\"\n",
    "\n",
    "def load_train(path):\n",
    "    train_set = pd.read_csv('./input/train_labels.csv')\n",
    "    train_label = np.array(train_set['invasive'].iloc[: ])\n",
    "    train_files = []\n",
    "    for i in range(len(train_set)):\n",
    "        train_files.append(path + str(int(train_set.iloc[i][0])) +'.jpg')\n",
    "    train_set['name'] = train_files\n",
    "    return train_files, train_set, train_label\n",
    "\n",
    "train_files, train_set, train_label = load_train(path)\n",
    "\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "3acf64c8-5d75-49d7-9e4e-578067f0b29d",
    "_uuid": "1ef06e8b5391d5a2a8ccea97aad7cd45b6c5cef4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  invasive\n",
       "0     1       0.5\n",
       "1     2       0.5\n",
       "2     3       0.5\n",
       "3     4       0.5\n",
       "4     5       0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./input/test/\"\n",
    "\n",
    "def load_test(path):\n",
    "    test_set = pd.read_csv('./input/sample_submission.csv')\n",
    "    test_files = []\n",
    "    for i in range(len(test_set)):\n",
    "        test_files.append(path + str(int(test_set.iloc[i][0])) +'.jpg')\n",
    "    return test_files, test_set\n",
    "\n",
    "test_files, test_set = load_test(path)\n",
    "\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "65b7adf0-65d6-4763-9442-b1da1c6b8090",
    "_uuid": "a4e584f2db16d8ba9e58ad74437a28e488425aed"
   },
   "source": [
    "# Define CNN Model Architecture\n",
    "*Kaggle can't access the weights file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "4c72959d-a413-4a8c-8caa-855f52cb292c",
    "_uuid": "9fa3d8ba6655ff38a08d8742f2da53ca4abef30d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0904 16:42:42.976113 23284 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0904 16:42:42.996058 23284 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0904 16:42:43.001046 23284 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0904 16:42:43.033468 23284 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0904 16:42:43.034464 23284 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0904 16:42:43.070370 23284 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0904 16:42:43.422440 23284 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0904 16:42:44.239274 23284 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "W0904 16:43:05.607470 23284 deprecation.py:506] From D:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 800, 800, 3)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 800, 800, 3)       12        \n",
      "_________________________________________________________________\n",
      "inception_v3 (Model)         (None, 23, 23, 2048)      21802784  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 21,804,845\n",
      "Trainable params: 21,770,407\n",
      "Non-trainable params: 34,438\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "img_height = 800\n",
    "img_width = 800\n",
    "img_channels = 3\n",
    "img_dim = (img_height, img_width, img_channels)\n",
    "\n",
    "def inceptionv3(img_dim=img_dim):\n",
    "    input_tensor = Input(shape=img_dim)\n",
    "    base_model = InceptionV3(include_top=False,\n",
    "                   weights='imagenet',\n",
    "                   input_shape=img_dim)\n",
    "    \n",
    "    bn = BatchNormalization()(input_tensor)\n",
    "    x = base_model(bn)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, output)\n",
    "    return model\n",
    "\n",
    "model = inceptionv3()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7c87589b-c9e5-48ac-b674-8ee7e3daeb31",
    "_uuid": "13890f2f0a7066552fdab668c93cdc95ef02fb5b"
   },
   "source": [
    "# Train Model\n",
    "Here we use 5-fold cross-validation to train the model. Submission file is saved with the average of all folds. Additionally, prediction arrays are saved for each fold in case we want to hand-pick results from an individual fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "12be4cfa-0094-40e5-bbca-0f477e6ea403",
    "_uuid": "09697a7318bcd523a8080dd8d4474b598a58481a"
   },
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, epochs, img_size, x, y, test, n_fold, kf):\n",
    "    roc_auc = metrics.roc_auc_score\n",
    "    preds_train = np.zeros(len(x), dtype = np.float)\n",
    "    preds_test = np.zeros(len(test), dtype = np.float)\n",
    "    train_scores = []; valid_scores = []\n",
    "\n",
    "    i = 1\n",
    "    \n",
    "    # sklearn.kfold\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        x_train = x.iloc[train_index]; x_valid = x.iloc[test_index]\n",
    "        y_train = y[train_index]; y_valid = y[test_index]\n",
    "        \n",
    "        # data augmentation (rotate/flip)\n",
    "        def augment(src, choice):\n",
    "            if choice == 0:\n",
    "                # Rotate 90\n",
    "                src = np.rot90(src, 1)\n",
    "            if choice == 1:\n",
    "                # flip vertically\n",
    "                src = np.flipud(src)\n",
    "            if choice == 2:\n",
    "                # Rotate 180\n",
    "                src = np.rot90(src, 2)\n",
    "            if choice == 3:\n",
    "                # flip horizontally\n",
    "                src = np.fliplr(src)\n",
    "            if choice == 4:\n",
    "                # Rotate 90 counter-clockwise\n",
    "                src = np.rot90(src, 3)\n",
    "            if choice == 5:\n",
    "                # Rotate 180 and flip horizontally\n",
    "                src = np.rot90(src, 2)\n",
    "                src = np.fliplr(src)\n",
    "            return src\n",
    "\n",
    "        def train_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(x_train), batch_size):\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "                    end = min(start + batch_size, len(x_train))\n",
    "                    train_batch = x_train[start:end]\n",
    "                    for filepath, tag in train_batch.values:\n",
    "                        img = cv2.imread(filepath)\n",
    "                        img = cv2.resize(img, img_size)\n",
    "                        img = augment(img, np.random.randint(6))\n",
    "                        x_batch.append(img)\n",
    "                        y_batch.append(tag)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    y_batch = np.array(y_batch, np.uint8)\n",
    "                    yield x_batch, y_batch\n",
    "\n",
    "        def valid_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(x_valid), batch_size):\n",
    "                    x_batch = []\n",
    "                    y_batch = []\n",
    "                    end = min(start + batch_size, len(x_valid))\n",
    "                    valid_batch = x_valid[start:end]\n",
    "                    for filepath, tag in valid_batch.values:\n",
    "                        img = cv2.imread(filepath)\n",
    "                        img = cv2.resize(img, img_size)\n",
    "                        img = augment(img, np.random.randint(6))\n",
    "                        x_batch.append(img)\n",
    "                        y_batch.append(tag)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    y_batch = np.array(y_batch, np.uint8)\n",
    "                    yield x_batch, y_batch\n",
    "\n",
    "        def test_generator():\n",
    "            while True:\n",
    "                for start in range(0, len(test), batch_size):\n",
    "                    x_batch = []\n",
    "                    end = min(start + batch_size, len(test))\n",
    "                    test_batch = test[start:end]\n",
    "                    for filepath in test_batch:\n",
    "                        img = cv2.imread(filepath)\n",
    "                        img = cv2.resize(img, img_size)\n",
    "                        x_batch.append(img)\n",
    "                    x_batch = np.array(x_batch, np.float32) / 255.\n",
    "                    yield x_batch\n",
    "\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=1e-4),\n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, cooldown=1, \n",
    "                               verbose=1, min_lr=1e-7),\n",
    "             ModelCheckpoint(filepath='./model/inception.fold_' + str(i) + '.hdf5', verbose=1,\n",
    "                             save_best_only=True, save_weights_only=True, mode='auto')]\n",
    "\n",
    "        train_steps = len(x_train) / batch_size\n",
    "        valid_steps = len(x_valid) / batch_size\n",
    "        test_steps = len(test) / batch_size\n",
    "        \n",
    "        model = model\n",
    "\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', \n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "        model.fit_generator(train_generator(), train_steps, epochs=epochs, verbose=1, \n",
    "                            callbacks=callbacks, validation_data=valid_generator(), \n",
    "                            validation_steps=valid_steps)\n",
    "\n",
    "        model.load_weights(filepath='./model/inception.fold_' + str(i) + '.h5')\n",
    "\n",
    "        print('Running validation predictions on fold {}'.format(i))\n",
    "        preds_valid = model.predict_generator(generator=valid_generator(),\n",
    "                                      steps=valid_steps, verbose=1)[:, 0]\n",
    "\n",
    "        print('Running train predictions on fold {}'.format(i))\n",
    "        preds_train = model.predict_generator(generator=train_generator(),\n",
    "                                      steps=train_steps, verbose=1)[:, 0]\n",
    "\n",
    "        valid_score = roc_auc(y_valid, preds_valid)\n",
    "        train_score = roc_auc(y_train, preds_train)\n",
    "        print('Val Score:{} for fold {}'.format(valid_score, i))\n",
    "        print('Train Score: {} for fold {}'.format(train_score, i))\n",
    "\n",
    "        valid_scores.append(valid_score)\n",
    "        train_scores.append(train_score)\n",
    "        print('Avg Train Score:{0:0.5f}, Val Score:{1:0.5f} after {2:0.5f} folds'.format\n",
    "              (np.mean(train_scores), np.mean(valid_scores), i))\n",
    "\n",
    "        print('Running test predictions with fold {}'.format(i))\n",
    "\n",
    "        preds_test_fold = model.predict_generator(generator=test_generator(),\n",
    "                                              steps=test_steps, verbose=1)[:, -1]\n",
    "\n",
    "        preds_test += preds_test_fold\n",
    "\n",
    "        print('\\n\\n')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i <= n_fold:\n",
    "            print('Now beginning training for fold {}\\n\\n'.format(i))\n",
    "        else:\n",
    "            print('Finished training!')\n",
    "\n",
    "    preds_test /= n_fold\n",
    "\n",
    "\n",
    "    return preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "75775f5a-2d75-4e64-88f4-b8b713393765",
    "_uuid": "709a92be4fadeb381414491e1029ad03a188a385"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 188 layers into a model with 3 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-afb0b72a15bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m test_pred = train_model(model, batch_size, epochs, img_size, train_set, \n\u001b[1;32m----> 8\u001b[1;33m                         train_label, test_files, n_fold, kf)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'invasive'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-c66b01e9775f>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, batch_size, epochs, img_size, x, y, test, n_fold, kf)\u001b[0m\n\u001b[0;32m     97\u001b[0m                       metrics = ['accuracy'])\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./model/inception.fold_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         model.fit_generator(train_generator(), train_steps, epochs=epochs, verbose=1, \n\u001b[0;32m    101\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1164\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[1;32m-> 1166\u001b[1;33m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers, reshape)\u001b[0m\n\u001b[0;32m   1028\u001b[0m                          \u001b[1;34m'containing '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m                          \u001b[1;34m' layers into a model with '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1030\u001b[1;33m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m     \u001b[1;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to load a weight file containing 188 layers into a model with 3 layers."
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "epochs = 50\n",
    "n_fold = 5\n",
    "img_size = (img_height, img_width)\n",
    "kf = KFold(n_splits=n_fold, shuffle=True)\n",
    "\n",
    "test_pred = train_model(model, batch_size, epochs, img_size, train_set, \n",
    "                        train_label, test_files, n_fold, kf)\n",
    "\n",
    "test_set['invasive'] = test_pred\n",
    "test_set.to_csv('./output/submission.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4013acf0-550f-46d5-a8f8-ef404ba7204b",
    "_uuid": "4ce0e9ea7237b6000f5d1c4df040543bfe7763ad"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
